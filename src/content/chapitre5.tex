\chapter{Une architecture d'habitats intelligents distribuée}
\label{chap:5}

\section{Introduction}

Il a été vu, dans les chapitres précédents qu'au fil des ans, diverses architectures de maisons intelligentes ont été proposées et mises en place au sein de laboratoires ou dans de réelles habitations pour mener des activités de recherche \citep{DJCook2003,Helal2005,Giroux2009,Cook2013,Bouchard2014,Lago2017,Plantevin2018}. Ces travaux se sont particulièrement intéressés à l'utilisation de capteurs, d'effecteurs et de techniques d'apprentissage machine pour la mise en application de l'\acl{IAm} comme méthode empirique afin de soutenir l'autonomie des personnes âgées et le suivi de la santé. Cependant, bien que chacun de ces travaux se distingue des autres, tous proposent différentes méthodes pour résoudre le problème principal qu'il cherchent à résoudre demeure la reconnaissance d'activités. Les avantages et les inconvénients de chaque architecture d'habitats intelligents ont déjà été discutés dans cette thèse et le principal problème qui a été identifié réside dans le manque de fiabilité et l'évolutivité de ces implémentations. En effet, la plupart des architectures admettent au moins un point de défaillance unique (\acl{SPOF} ou \acs{SPOF}) principalement à cause de leur implémentation centralisée, ou \og monolithique \fg. Néanmoins, il a aussi été montré que \cite{Plantevin2018} ont adressé cette problématique en introduisant un nouveau type d'architecture basée sur l'utilisation de transducteurs intelligents distribués. Cependant, bien que cette implémentation ait été pensée pour être une architecture de maisons intelligentes prête à l'emploi en situation réelle d'utilisation, sa conception ne semble pas suffisamment adaptée aux environnements de recherche. En effet, la simplicité de mise en place de nouvelles méthodes pour la reconnaissance d'activités et de nouveaux protocoles expérimentaux semble avoir été oubliés dans cette implémentation.

Par ailleurs, dans la comparaison des différentes architectures de maisons intelligentes proposées au chapitre \ref{chap:2}, le principal inconvénient des solutions existantes qui a été identifié concerne leur manque de flexibilité quant à l'intégration de nouveaux matériels et plus spécifiquement, les \textit{wearable devices}. En effet, pour réaliser la reconnaissance d'activités, la plupart des méthodes proposées avec ses dispositifs sont des applications complètes. La plupart du temps, les différents processus qui composent la reconnaissance sont encapsulés au sein d'un unique composant logiciel immuable. Il devient donc difficile de les modifier. De plus, le fait de disposer d'une unique application ne favorise pas la réutilisation de certains mécanismes communs à plusieurs méthodes. Ceux-ci doivent donc généralement être soit développés à nouveau, soit adaptés afin de pouvoir supporter des modifications dans la méthode proposée. De surcroît, l'exploitation de ce genre d'application \og prête à l'emploi \fg est, la plupart du temps, complexifiée. En effet, selon les dépendances, le langage de programmation et la plateforme, elles peuvent être difficiles à adapter et à déployer d'un environnement à un autre (\textit{p. ex.} entre deux laboratoires de recherche différents).

Ainsi, ce travail présente un autre type d'architecture de maisons intelligentes qui se veut fiable et évolutive dont l'implémentation est inspirée des architectures de \textit{cloud} privées sur site. Cette architecture vise à résoudre la plupart des problèmes identifiés au sein des implémentations précédemment proposées sans qu'il soit nécessaire de remplacer leur structure au complet. En effet, l'implémentation présentée dans ce chapitre a été pensée pour à la fois être déployée en l'état, ou pour s'intégrer aux architectures existantes, remplaçant ainsi les entités centrales des architectures monolithiques. L'objectif principal de cette implémentation est alors de pouvoir y déployer facilement de nouveaux composants logiciels expérimentaux indépendamment des différences dans leur conception, tant en termes de langages de programmation que de librairies requises ou de plateforme supportées. Ceci dans le but de favoriser l'interopérabilité entre les différents processus, leur réutilisation ainsi que leur mise à l'échelle vis-à-vis de la quantité de ressources requise.

La suite de ce chapitre comporte une première section qui présente en détail l'architecture d'habitats intelligents proposée. Ensuite, les expérimentations sont décrites dans une seconde section. De plus, cette section propose une discussion des observations réalisées. Finalement, dans une dernière partie, une conclusion est dressée quant à ce second travail.

\section{Architecture Proposée}

L'architecture introduite dans ce chapitre a été conçue pour être compatible avec la majorité des architectures de maisons intelligentes présentes dans la littérature. Par conséquent, celle-ci peut être considérée comme une extension du travail proposé par \cite{Plantevin2018}, puisqu'ils ont concentré leurs efforts sur la suppression des \acsp{SPOF} dans le but d'améliorer la fiabilité globale des habitats intelligents existants. Néanmoins, l'idée principale de ce travail est d'aller plus loin dans l'idée de rendre ces architectures plus fiables et plus flexibles. Pour ce faire, ce chapitre présente une implémentation qui repose sur l'utilisation des microservices plutôt qu'une approche monolithique.

\subsection{Microservices}

Les architectures de microservices sont récemment apparues comme un nouveau paradigme pour la programmation d'applications et dont l'origine se base sur le concept d'\ac{AOS} \citep{MacKenzie2006}. Comme le montre la figure \ref{fig:mono_micro}, cette approche suggère de diviser une application en un ensemble de services plus petits, indépendants et interconnectés. En comparaison aux applications monolithiques, où tous les composants logiciels se trouvent dans une seule instance, les services exécutent des fonctions plus détaillées et précises. Bien que les architectures monolithiques soient simples à implémenter, les architectures de microservices présentent, elles aussi, différents avantages. La première concerne l'\emph{agilité} des microservices. En effet, puisque les applications sont fragmentées au niveau le plus élémentaire de leurs fonctionnalités, chaque service devient individuellement plus facile à maintenir et beaucoup plus rapide à développer et à déployer. Le second avantage principal demeure la \emph{réutilisabilité}. Les microservices permettent aux développeurs de créer des applications en utilisant certains fragments déjà existants. De plus, la barrière entre les technologies (\textit{p. ex.} les langages de programmation) est réduite puisque les services fonctionnent de manière indépendante. En ce sens, cet \emph{agnosticisme technologique} permet d'obtenir des applications hétérogènes du point de vue des technologies dans leur conception. Par ailleurs, un autre avantage offert par les architectures de microservices concerne la mise à l'échelle des différents composants à mesure que la demande pour une application augmente. En cas de forte demande ponctuelle, il est possible soit d'augmenter la quantité de ressources allouées, soit d'augmenter le nombre d'instances pour les services les plus impactés, plutôt que pour l'intégralité de application. Il s'agit respectivement de l'\emph{extensibilité dynamique horizontale} et de l'\emph{extensibilité dynamique verticale}. Enfin, puisque les composants d'une application dans une architecture de microservices sont généralement distribués, l'application est en mesure de supporter les pannes au niveau des services qui la composent. En effet, lorsqu'un service est en panne, la fonctionnalité dont il est responsable devient inopérante, mais le fonctionnement du reste de l'application n'est pas altéré. De plus, il est possible de mettre en place différents mécanismes pour remédier aux défaillances inattendues afin d'augmenter la résilience et la \emph{fiabilité} de l'ensemble du système.

\begin{figure}[t]
	\centering
	\includegraphics[width=.8\linewidth]{chapter5/mono_micro.pdf}
		\caption{Comparaison entre les architectures monolithiques et de microservices selon un exemple d'application qui vise à résoudre un problème d'apprentissage machine par le biais d'une interface graphique.}
	\label{fig:mono_micro}
\end{figure}

\begin{figure}[hb!]
	\centering
	\includegraphics[width=.8\linewidth]{chapter5/containers_vms.pdf}
		\caption{Illustration détaillée des trois principales techniques de virtualisation pour la mise en place d'une architecture de microservices.}
	\label{fig:containers_vms}
\end{figure}

Actuellement, il existe de nombreuses façons de mettre en place une architecture de microservices, la plus courante étant l'utilisation d'outils de virtualisation tels que les machines virtuelles (\acsp{VM}) traditionnelles, les conteneurs ou les conteneurs à l'intérieur de \acsp{VM}. La figure \ref{fig:containers_vms} fournit une représentation graphique de ces trois différentes implémentations. Même si le choix de la méthode de virtualisation n'affecte pas les possibilités offertes par les architectures de microservices, il existe des différences significatives entre l'utilisation de \acsp{VM} traditionnelles et les conteneurs qu'il semble important de détailler. La principale différence réside dans le fait que les conteneurs proposent une virtualisation au niveau du système d'exploitation alors que les \acsp{VM} offrent une virtualisation au niveau du matériel. En ce qui concerne les \acsp{VM}, l'hyperviseur permet de cloisonner des portions du matériel. En effet, son rôle est d'attribuer à chaque machine virtuelle les ressources nécessaires en fonction des ressources physiques du système hôte. De manière générale, il existe deux types d'hyperviseurs. Dans un premier temps, il y a les hyperviseurs de système natif qui s'exécutent directement sur le matériel du système hôte. En ce sens, ceux-ci prennent la place du système d'exploitation de l'hôte et planifient directement l'utilisation des ressources allouées aux machines virtuelles en fonction du matériel. Dans un second temps, les hyperviseurs hébergés fonctionnent comme une couche logicielle supplémentaire au sein du système d'exploitation hôte. Ainsi, ils permettent de dissocier les systèmes d'exploitation invités (virtualisés) du système d'exploitation hôte.

Alternativement, les conteneurs permettent aux instances virtuelles de partager un système d'exploitation hôte unique. Par conséquent, puisque les conteneurs n'ont pas à embarquer un système d'exploitation, les images virtualisées demeurent beaucoup plus légères que celles des \acsp{VM} traditionnelles. De plus, les conteneurs sont, au même titre que les \acsp{VM}, isolés du système hôte, c'est-à-dire, qu'ils s'exécutent dans des espaces séparés à la fois les uns des autres, mais également de certaines parties du système d'exploitation hôte. Une telle isolation permet donc une utilisation plus efficace des ressources. Enfin, les conteneurs sont très rapides à créer et à détruire puisqu'il n'est pas nécessaire de démarrer ou d'arrêter un système d'exploitation à chaque fois. En effet, les conteneurs s'occupent uniquement de compléter le processus dont ils sont en charge.

En somme, une \acs{VM} est une émulation d'un système informatique tandis que les conteneurs sont toutes les unités logiciel qui regroupent le code et toutes ses dépendances requises (\textit{p. ex.} les librairies ou les binaires) qui composent une application. De ce fait, celle-ci peut s'exécuter de façon fiable et rapide d'un environnement à un autre. Ces deux technologies ne sont pas incompatibles puisqu'il est possible de les utiliser ensemble. Cependant, l'exécution de conteneurs à l'intérieur de machines virtuelles est, la plupart du temps, le résultat de l'évolution d'environnements de virtualisation matures déjà existants. Généralement, les conteneurs sont exécutés directement sur le système hôte afin d'optimiser les performances et la latence ou de réduire les coûts des licences des outils de virtualisation et des systèmes d'exploitation.

\subsection{Organisation Matérielle}

\begin{figure}[t]
	\centering
	\includegraphics[width=.9\linewidth]{chapter5/proposed_arch.pdf}
        \caption{Organisation matérielle de l'architecture d'habitats intelligents proposée illustrant l'implémentation d'un \textit{cluster} pour le support de l'architecture de microservices ainsi que le système de fichiers distribué.}
	\label{fig:proposed_arch}
\end{figure}

L'organisation matérielle de l'implémentation de l'architecture d'habitats intelligents présentée dans ce chapitre est illustrée par la figure \ref{fig:proposed_arch}. Afin de supporter l'architecture de microservices, cette organisation repose sur un ensemble de cinq n\oe{}uds principaux (\texttt{m0}, \texttt{m1}, \texttt{m2}, \texttt{w0} et \texttt{w1}) qui forment un \textit{cluster} distribué. Par ailleurs, l'implémentation proposée suggère l'utilisation de deux n\oe{}uds supplémentaires (\texttt{f0} et \texttt{f1}) sur chacun desquels une unité de stockage de type \acs{RAID} (\acl{RAID}) est interfacée. L'objectif de ces n\oe{}uds est de fournir un système de fichiers distribué à l'ensemble du \textit{cluster}. En effet, puisque certains conteneurs ont la capacité de monter différents volumes sur le système hôte, un tel système de fichiers offre un moyen fiable de partager des données entre chaque conteneur, quel que soit le n\oe{}ud sur lequel ils s'exécutent. En outre, tous les n\oe{}uds qui composent le \textit{cluster} et le système de fichier distribué sont connectés à un commutateur réseau pour leur permettre de communiquer entre eux. De plus, l'ensemble de l'architecture est isolé à l'intérieur d'un réseau local afin de prévenir les problèmes de sécurité qui pourraient compromettre les expériences et de préserver l'anonymat des participants. Toutefois, pour faciliter la configuration et la maintenance de l'architecture, cette dernière demeure accessible de manière ad hoc avec un ordinateur \textit{via} un routeur qui est, quant a lui, connecté à la fois au Web et au réseau local.

\subsection{Système de fichiers hautement disponible}

Le système de fichiers distribué fourni par les n\oe{}uds \texttt{f0} et \texttt{f1} repose sur le protocole \ac{NFS}. Le principe de ce protocole reste simple, car il est implémenté selon le modèle client/serveur. Un serveur \acs{NFS} gère l'authentification, l'autorisation, l'administration des clients ainsi que les données. Ainsi, une fois autorisés, les clients NFS peuvent accéder aux données distantes de la même manière qu'ils le font localement. En ce sens, chaque n\oe{}ud qui compose le \textit{cluster} est autorisé à accéder (lecture) et à manipuler les fichiers (création, mise à jour, suppression) de manière classique. Néanmoins, tous les autres n\oe{}uds seront informés des changements qui en résultent.

Cependant, puisque l'idée est de proposer une architecture fiable, le fait de n'avoir qu'un seul serveur \acs{NFS} ne satisfaisait pas cette contrainte. En ce sens, l'outil \acs{DRDB}\footnote{\url{https://www.linbit.com/drbd/}} (\acl{DRDB}) a été mis en place afin que les n\oe{}uds \texttt{f0} et \texttt{f1} puissent proposer un système de fichiers à la fois distribué, mais aussi répliqué. \acs{DRDB} a été paramétré pour effectuer une réplication complète des données stockées entre le serveur principal (\texttt{f0}) et le serveur secondaire (\texttt{f1}), soit le même fonctionnement qu'un système de stockage de type \acs{RAID} 1 en réseau. Afin de favoriser l'intégrité des données plutôt que la vitesse de réplication, la mise en place d'une réplication synchrone constante entre le n\oe{}ud principal et le n\oe{}ud secondaire a été adoptée plutôt qu'une réplication asynchrone ou semi-synchrone. Ainsi, les opérations de manipulation des fichiers réalisées sur le n\oe{}ud principal ne sont considérées comme terminées que lorsque les écritures sur le disque local et sur le disque distant du n\oe{}ud secondaire sont confirmées par ce dernier. Néanmoins, \acs{DRDB} ne fournit aucune fonctionnalité relative à un quelconque mécanisme garantissant la fiabilité du système de fichiers en cas de panne d'un des deux n\oe{}uds. En ce sens, l'outil Heartbeat\footnote{\url{http://www.linux-ha.org/doc/man-pages/re-heartbeat.html}} a également été adopté dans cette implémentation. Cet outil permet de proposer un mécanisme permettant de garantir la haute disponibilité du système de fichiers distribué. En tant que processus d'arrière-plan, le logiciel vise à surveiller l'état des deux serveurs \acs{DRDB}. En cas de défaillance du n\oe{}ud principal (\texttt{f0}), l'adresse \ac{VIP} du système de fichiers distribué qui pointe vers \texttt{f0} est automatiquement réaffectée pour identifier le n\oe{}ud secondaire (\texttt{f1}). Ainsi, l'accès aux données n'est en aucun cas compromis lors de la perte d'un seul n\oe{}ud, puisque cette opération est effectuée de manière transparente pour les clients \acs{NFS}.

\subsection{Orchestration des conteneurs}

L'implémentation de l'architecture de microservices proposée dans ce chapitre repose d'abord sur le moteur de conteneurs Docker\footnote{\url{https://www.docker.com/}}, qui a été installé sur chaque n\oe{}ud qui compose le \textit{cluster} (\texttt{m0}, \texttt{m1}, \texttt{m2}, \texttt{w0} et \texttt{w1}). De plus, afin de pouvoir mettre en place le \textit{cluster} distribué grâce à l'ensemble de ces n\oe{}uds, il était nécessaire d'utiliser un orchestrateur. L'orchestration de conteneurs fait référence à un ensemble de techniques qui permettent de gérer les différents cycles de vie des conteneurs ainsi que leurs environnements dynamiques. Plus précisément, les outils d'orchestration permettent d'automatiser plusieurs tâches telles que la configuration, l'ordonnancement, l'approvisionnement et la mise à l'échelle des conteneurs, mais également la répartition de charge, l'allocation des ressources et la sécurité des interactions entre les conteneurs. Il existe actuellement plusieurs outils d'orchestration de conteneurs populaires tels que Kubernetes\footnote{\url{https://kubernetes.io/}}, Apache Mesos\footnote{\url{https://mesos.apache.org/}} et Docker Swarm\footnote{\url{https://docs.docker.com/engine/swarm/}}. Pour notre architecture, c'est l'outil Docker Swarm qui a été adopté. Plusieurs facteurs ont motivé ce choix. Tout d'abord, cet outil est plus simple à mettre en place puisqu'il est conditionné, distribué et totalement intégré au moteur de conteneur qui est déjà utilisé. En outre, bien qu'il est considéré comme moins extensible parmi les trois outils mentionnés, celui-ci offre toutes les fonctionnalités les plus importantes requises pour créer et gérer le \textit{cluster} de cette architecture.

De manière générale, un \textit{cluster} Docker Swarm est composé de plusieurs n\oe{}uds sur lesquels le moteur Docker est installé. Ces n\oe{}uds peuvent adopter deux types de rôles\textemdash un rôle de gestionnaire (\textit{manager}) ou un rôle de travailleur (\textit{worker}). Les n\oe{}uds de type \textit{manager} sont ceux qui reçoivent les fichiers de définition des services à exécuter au sein du \textit{cluster}. De plus, les \textit{managers} distribuent les tâches qui composent ces services aux n\oe{}uds de type \textit{worker}, en fonction de leurs fichiers de définition. Ainsi, ce sont les n\oe{}uds \textit{manager} qui sont responsable de l'orchestration et de la gestion du \textit{cluster}. Dans ce contexte, les services font référence à un ensemble de tâches, c'est-à-dire des conteneurs, qui doivent s'exécuter sur les n\oe{}uds travailleurs du \textit{cluster} pour réaliser un processus donné. Néanmoins, il est tout à fait possible de définir une politique de placement manuellement. Ainsi, un service peut être soit global, soit répliqué. Lorsqu'un service est global, une unique instance est déployée sur chaque n\oe{}ud du \textit{cluster} qui est disponible, et ce, sans tenir compte de son rôle.  À l'inverse, lorsqu'un service est répliqué, il est nécessaire de préciser, dans le fichier de définition, le nombre de répliques qui doivent être déployées ainsi que le type de n\oe{}uds sur lesquelles elles seront exécutées. Un des n\oe{}uds \textit{manager} est alors en charge d'établir cette répartition. Par ailleurs, afin que les n\oe{}uds \textit{manager} puissent suivre l'évolution de l'exécution des services, les n\oe{}uds travailleurs incluent un agent dont le rôle est de rendre compte de l'état des services qui lui sont attribués aux \textit{managers}. Par défaut, les n\oe{}uds gestionnaires sont également des n\oe{}uds travailleurs, mais il est possible de les configurer de manière à ce qu'ils n'acceptent aucune charge de travail en plus de leurs tâches de gestion du \textit{cluster}.

En ce qui concerne les \textit{managers}, puisqu'ils représentent les n\oe{}uds les plus importants pour la stabilité du \textit{cluster}, il est nécessaire que ce dernier en comporte plusieurs. Plus précisément, préférablement un nombre impair, selon les recommandations préconisées par Docker Swarm. En effet, parmi tous les n\oe{}uds de type \textit{manager} l'un d'entre eux est élu en tant que leader par tous les autres \textit{managers} selon une implémentation de l'algorithme de consensus Raft \citep{MacKenzie2006}. Ainsi, ce n\oe{}ud particulier est responsable de toutes les décisions pour l'ensemble du groupe de \textit{managers}. Bien qu'il n'y ait pas de limite quant au nombre de n\oe{}uds gestionnaires que peut admettre un \textit{cluster} Docker Swarm, la décision concernant l'effectif à mettre en place demeure un compromis entre la performance et la tolérance aux pannes. En effet, si le n\oe{}ud élu comme leader du \textit{cluster} devient indisponible, n'importe quel autre \textit{manager} peut obtenir ce rôle si le quorum Raft\textemdash c'est-à-dire, un accord majoritaire entre les n\oe{}uds gestionnaires est possible. Plus précisément, l'algorithme Raft tolère jusqu'à $(N -1)/2$ défaillances et exige un quorum de $(N/2) + 1$ membres pour permettre une décision, où $N$ fait référence au nombre de n\oe{}uds \textit{manager}. Il apparait donc clairement qu'un tel processus peut créer une surcharge du trafic réseau avec un nombre élevé de \textit{managers}, impactant alors la performance globale du \textit{cluster}. En ce sens, nous avons opté pour un \textit{cluster} de cinq n\oe{}uds, soit trois gestionnaires et deux travailleurs. Ceci dans le but de tolérer une défaillance complète d'un n\oe{}ud, quel que soit son rôle. De plus, un tel nombre de \textit{managers} permet de respecter la contrainte d'avoir un nombre impair de n\oe{}uds gestionnaires pour que le quorum soit toujours valide en cas de panne.

Pour permettre aux conteneurs de communiquer entre eux au sein du \textit{cluster} Docker Swarm, un pilote réseau spécifique a été utilisé. Ce pilote permet de créer des réseaux superposés en s'appuyant sur la technologie de tunnelisation \ac{VXLAN} \citep{rfc7348} qui permet l'encapsulation de trames réseau de la couche 2 dans \acs{UDP} (couche 4). Ainsi, dans un contexte de virtualisation comme celui imposé par les conteneurs, ce pilote permet de simplifier la complexité de la gestion des différents réseaux virtuels nécessaires à la communication entre les conteneurs qui sont déployés de manière éparse sur les différents n\oe{}uds du \textit{cluster}. De plus, les réseaux superposés offrent des mécanismes qui permettent de s'assurer du bon fonctionnement de la redondance d'un point de vue réseau au sein du \textit{cluster}. Lorsqu'un \textit{cluster} Docker Swarm est initialisé, un réseau \textit{ingress} est automatiquement créé sur chaque n\oe{}ud. Il s'agit d'un réseau superposé particulier dont l'objectif est de faciliter la répartition de la charge des différents services entre les n\oe{}uds du \textit{cluster}. Il est ensuite possible de créer autant de réseaux superposés que nécessaire pour interconnecter les conteneurs qui ont besoin d'échanger des données. Par ailleurs, un réseau de type pont (\textit{bridge network}) est également créé automatiquement sur chaque n\oe{}ud lors de l'initialisation du \textit{cluster}. Ces réseaux sont nécessaires pour relier tous les réseaux superposés (y compris le réseau \textit{ingress}) au réseau physique des n\oe{}uds du \textit{cluster}. Par conséquent, comme les réseaux de type pont se situent au-dessus de la couche réseau du système hôte, une communication entre les conteneurs interconnectés par un réseau superposé, mais s'exécutant dans des n\oe{}uds différents est rendue possible. Par défaut, ces communications sont chiffrées à l'aide de l'algorithme AES-GCM \citep{rfc5288}. Les n\oe{}uds \textit{manager} du cluster assurent le remplacement de la clé de chiffrement qu'ils partagent toutes les douze heures pour assurer un niveau de sécurité acceptable sans compromettre les performances des communications réseau.

Puisque les réseaux superposés attribuent dynamiquement une adresse \acl{VIP} à chaque conteneur lors de leur création, ceux-ci sont isolés d'un point du vu réseau ce qui simplifie donc la consommation des services. Toutefois, lorsque le nombre de services devient important, il peut sembler pertinent de connaître à l'avance certaines \acsp{VIP}, ce qui n'est évidemment pas possible. Pour résoudre ce problème, l'orchestrateur Swarm propose un mécanisme de découverte de services. Cette fonctionnalité repose sur un serveur DNS qui est intégré dans le moteur de conteneur de chaque n\oe{}ud. Ce dernier est alors chargé de résoudre le nom des services pour permettre le routage des requêtes réseau vers les conteneurs correspondants grâce à leurs \acsp{VIP} associées.  En d'autres termes, le nom d'un service donné peut par exemple, être utilisé en remplacement de son adresse \acl{VIP} dans un fichier de configuration. Ainsi, cette référence sera remplacée correctement interprétée grâce au processus de découverte de services une fois que la \acs{VIP} sera connue. Néanmoins, lorsque les services sont répliqués, le mécanisme de découverte de services seul n'est plus suffisant. En ce sens, grâce à l'orchestrateur,  un mécanisme de répartition est mis en place dans le but de déterminer quelle réplique d'un service en particulier est utilisée. Ainsi, Docker Swarm s'appuie sur le protocole \ac{IPVS}, une implémentation de la répartition de charge qui est déjà intégrée au niveau de la couche réseau transport du noyau Linux. Grâce à ce protocole, les requêtes \acs{TCP}/\acs{UDP} inter-services peuvent donc être redirigée correctement vers les conteneurs appropriés. Dans le cas spécifique d'un \textit{cluster} Docker Swarm, chaque n\oe{}ud écoute sur les ports qui sont exposés par les services qui doivent être accessibles à distance depuis un environnement extérieur (\textit{p. ex.} un tableau de bord web qui écoute sur le port \texttt{8080}). Ensuite, \acs{IPVS} applique une répartition de la charge selon l'algorithme Round-Robin \citep{Ghaffarinejad2014} pour transmettre la demande à l'une des répliques actives du service en question. Comme le montre la figure \ref{fig:swarm_load_balancing}, si le n\oe{}ud sollicité ne contient aucune réplique du service demandé par un client, alors \acs{IPVS} va router la requête à une réplique active présente sur un autre n\oe{}ud, ce qui est possible grâce au réseau superposé de type \textit{ingress}.

\begin{figure}[t]
	\centering
	\includegraphics[width=.8\linewidth]{chapter5/swarm_load_balancing.pdf}
        \caption{Exemple du fonctionnement de la répartition de la charge selon l'algorithme Round-Robin appliquée par le protocole \acs{IPVS} sur lequel repose Docker Swarm où un des itinéraires possibles pour une requête faite sur le service répliqué \texttt{App} est identifié par des flèches rouges.}
	\label{fig:swarm_load_balancing}
\end{figure}

\subsection{Proxy inverse}

Puisque notre architecture est composée d'un \textit{cluster} de cinq n\oe{}uds, tous les services qui y sont déployés demeurent accessibles \textit{via} cinq points d'entrée différents, étant donné que l'orchestrateur ouvre tous ports de ces services sur chacun des n\oe{}uds du \textit{cluster}. Cependant, avec un nombre important de services, il peut s'avérer difficile de gérer quel service est publié sur quel port. Ainsi,la répartition de la charge au niveau des services est effectuée \textit{via} l'orchestrateur, il est apparu indispensable de proposer une répartition de la charge également au niveau des n\oe{}uds. Ceci dans le but de proposer un accès externe unifié et sécurisé aux services internes par le biais d'une \acs{URL} (\acl{URL}) telle que \texttt{https://service.domain.com}. Pour ce faire, l'outil open-source Tr\ae{}fik\footnote{\url{https://traefik.io/}} a été préféré à d'autres options telles que NGINX\footnote{\url{https://www.nginx.com/}} ou HAProxy\footnote{\url{https://www.haproxy.org/}}, car il est le mieux adapté aux architectures basées sur des microservices. En effet, le principal avantage de Tr\ae{}fik est sa capacité à découvrir automatiquement les informations réseau et les services disponibles dans le \textit{cluster} et à mettre à jour dynamiquement sa configuration en fonction de l'évolution de l'environnement. Ces fonctionnalités sont particulièrement importantes avec l'utilisation de  microservices, car ceux-ci sont généralement sans état (\textit{stateless}), mais aussi, car leur durée de vie est souvent courte et de nouvelles versions peuvent avoir à être déployées fréquemment. De plus, leurs instances peuvent être mises à l'échelle de façon dynamique en fonction de l'augmentation de la demande.

\begin{figure}[hb!]
	\centering
	\includegraphics[width=.8\linewidth]{chapter5/arch_reverse_proxy.pdf}
        \caption{Exemple du fonctionnement du proxy inverse Tr\ae{}fik lorsqu'une requête pour accéder au service \texttt{App} est effectuée par un client \textit{via} une \acs{URL} sécurisée.}
	\label{fig:arch_reverse_proxy}
\end{figure}

Tr\ae{}fik est considéré comme un proxy inverse, c'est-à-dire, le principal point d'entrée d'un réseau privé. En effet, les proxys inverses sont, la plupart du temps, placé derrière un pare-feu et leur rôle est d'acheminer les requêtes entrantes des clients vers un des n\oe{}uds du \textit{cluster} et le service correspondant. En outre, les proxys inverses peuvent également remplir un certain nombre d'autres fonctions importantes pour améliorer l'efficacité et la sécurité des systèmes sur lesquels ils sont mis en place. Tout d'abord, en ce qui concerne Tr\ae{}fik, ce dernier fournit une répartition de la charge au niveau des n\oe{}uds ce qui permet de dispatcher les requêtes à l'ensemble des n\oe{}uds actifs au sein du \textit{cluster} pour éviter que l'un d'entre eux ne soit saturé. De plus, cet outil permet également de masquer l'organisation interne de l'architecture, car il apporte une couche supplémentaire de protection puisque l'adresse IP du n\oe{}ud qui traite réellement les requêtes n'est jamais dévoilée. Enfin, toujours dans une optique d'offrir une meilleure protection, les proxys inverses et pour cette implémentation, Tr\ae{}fik, permettent de chiffrer les échanges de données entre les clients et les services de l'architecture grâce à un ou plusieurs certificats \acs{TLS} associés aux différents services ou sous-domaines. La figure \ref{fig:arch_reverse_proxy} illustre un exemple de ce fonctionnement. En effet, Tr\ae{}fik propose un point d'entrée unique qui permet d'accéder au service \texttt{App} \textit{via} une \acs{URL} sécurisée : \texttt{https://app.liara.uqac.ca}. Ainsi lorsque cette requête est soumise par le client (\textit{p. ex. } un navigateur web), le répartiteur interne de Tr\ae{}fik achemine la requête vers l'une des répliques actives du service \texttt{App}. Dans ce scénario, la répartition de la charge fournie par l'orchestrateur Docker Swarm n'est pas affichée pour simplifier la figure, mais celle-ci demeure tout de même active et opérationnelle. En effet, bien que Tr\ae{}fik ait routé la requête vers le \texttt{n\oe{}ud 1}, il est possible pour le répartiteur de charge du \textit{cluster}, quant à lui, achemine en interne la demande vers la \texttt{réplique 0} du service \texttt{App} qui s'exécute sur le \texttt{n\oe{}ud 0}.

\subsection{Gestion du \textit{cluster}}

\subsection{Base de données répliquée}

\begin{figure}[H]
	\centering
	\includegraphics[width=.9\linewidth]{chapter5/mongodb_replication.pdf}
        \caption{caption}
	\label{fig:mongodb_replication}
\end{figure}

\subsection{Dépôt de conteneurs}

\subsection{Organisation des conteneurs}

\begin{figure}[H]
	\centering
	\includegraphics[width=.9\linewidth]{chapter5/containers.pdf}
        \caption{caption}
	\label{fig:containers}
\end{figure}


\section{Experimentations}

\subsection{Installation Matérielle}

\subsection{Haute disponibilité}

\section{Conclusion}